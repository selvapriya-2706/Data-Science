import pandas as pd
import gradio as gr
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder 
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor


# Load dataset
df = pd.read_csv('house_price_forecasting_dataset.csv')

# Display basic info
print("Dataset Head:")
print(df.head())

# Drop rows with missing values (simple handling)
df = df.dropna()

# Separate features and target
# Assuming 'Price' is the target column
X = df.drop('Price', axis=1)
y = df['Price']

# Convert categorical features to numeric if needed
X = pd.get_dummies(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model training
model = LinearRegression()
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.2f}")

import pandas as pd
# Read the dataset
df = pd.df.info()('/content/house_price_forecasting_dataset.csv', sep=';')

# Display first few rows
df.head()

# Shape of the dataset
print("Shape:", df.shape)

# Column names
print("Columns:", df.columns.tolist())

df.info()

# Summary statistics for numeric features
df.describe()

# Check for missing values
print(df.isnull().sum())
# Check for duplicates
print("Duplicate rows:", df.duplicated().sum())

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# Load dataset
df = pd.read_csv('house_price_forecasting_dataset.csv')

# Drop non-predictive ID column
df.drop('HouseID', axis=1, inplace=True)

# Convert categorical variables
df = pd.get_dummies(df, columns=['Location', 'Garage'], drop_first=True)

# ========== EDA ==========
# Histogram of target variable
sns.histplot(df['Price'], kde=True)
plt.title('Distribution of House Prices')
plt.xlabel('Price')
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Boxplot: Distance vs Price
sns.boxplot(x=pd.qcut(df['DistanceToCityCenter_km'], q=4), y=df['Price'])
plt.title('Distance to City Center vs Price')
plt.xticks(rotation=45)
plt.show()

# ========== Modeling ==========
# Feature/Target split
X = df.drop('Price', axis=1)
y = df['Price']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model 1: Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))
print(f"Random Forest RMSE: {rf_rmse:.2f}")

# Model 2: XGBoost
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))
print(f"XGBoost RMSE: {xgb_rmse:.2f}")

# Feature Importance (XGBoost)
plt.figure(figsize=(10, 6))
sns.barplot(x=xgb_model.feature_importances_, y=X.columns)
plt.title("Feature Importances (XGBoost)")
plt.show()

# Load the dataset
df = pd.read_csv('house_price_forecasting_dataset.csv')

# Drop ID column (not predictive)
df.drop(columns='HouseID', inplace=True)
# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_cols.tolist())

# One-hot encode categorical columns
df = pd.get_dummies(df, columns=['Location', 'Garage'], drop_first=True)

# Define target and features
target = 'Price'
features = df.columns.drop(target)
print("Features used for training:", features.tolist())

X = df[features]
y = df[target]

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use Gradient Boosting Regressor (smart technique)
model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, max_depth=5, random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
predictions = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, predictions))
print(f"Gradient Boosting RMSE: {rmse:.2f}")

# Identify categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
print("Categorical Columns:", categorical_cols.tolist())

# Load data
df = pd.read_csv('house_price_forecasting_dataset.csv')

# Drop ID column
df.drop(columns='HouseID', inplace=True)

# One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['Location', 'Garage'], drop_first=True)

# Define target and features
target = 'Price'
X = df_encoded.drop(columns=target)
y = df_encoded[target]

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train Ridge Regression (smart linear model with regularization)
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f'Ridge Regression RMSE: {rmse:.2f}')

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)
# Predict
y_pred = model.predict(X_test)

print("MSE:", mean_squared_error(y_test, y_pred))
print("RÂ² Score:", r2_score(y_test, y_pred))

df = pd.read_csv('house_price_forecasting_dataset.csv')

# 2. Drop unhelpful columns
df.drop(columns='HouseID', inplace=True)

# 3. One-hot encode categorical variables
df_encoded = pd.get_dummies(df, columns=['Location', 'Garage'], drop_first=True)

# 4. Define target and features
target = 'Price'
X = df_encoded.drop(columns=target)
y = df_encoded[target]

# 5. Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 6. Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 7. Smart model: Gradient Boosting Regressor
model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42)
model.fit(X_train, y_train)

# 8. Evaluate model
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE on test set: {rmse:.2f}")

# Load dataset
df = pd.read_csv("/content/house_price_forecasting_dataset.csv")

# Drop or fill missing values
df = df.dropna()  # or use df.fillna(method='ffill') or df.fillna(df.mean())

# Separate features and target
# The target column name is 'Price', not 'SalePrice'
X = df.drop("Price", axis=1)  # Changed 'SalePrice' to 'Price'
y = df["Price"]  # Changed 'SalePrice' to 'Price'

# One-hot encode categorical variables
X = pd.get_dummies(X, drop_first=True)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train regression model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Root Mean Squared Error: {rmse:.2f}")

# --- Forecast for new input ---
# Assuming new_house is a dictionary of input values for prediction
new_house = {
    'LotArea': 8450,
    'OverallQual': 7,
    'YearBuilt': 2003,
    'Neighborhood': 'CollgCr',
    'HouseStyle': '2Story',
    # ... include all required fields
}

# Convert to DataFrame
new_df = pd.DataFrame([new_house])

# Combine with original to match one-hot encoded columns
X_full = pd.concat([X, new_df], ignore_index=True)
X_full_encoded = pd.get_dummies(X_full, drop_first=True)

# Align columns with training set
X_full_encoded = X_full_encoded.reindex(columns=X.columns, fill_value=0)

# Scale the new input
new_input_scaled = scaler.transform(X_full_encoded.tail(1))

# Predict the price
predicted_price = model.predict(new_input_scaled)[0]
print(f"Predicted House Price: ${predicted_price:,.2f}")

# Load your dataset (replace 'data.csv' with your actual file)
data = pd.read_csv('/content/house_price_forecasting_dataset.csv')

# Assume 'Price' is the target variable and other columns are features
X = data.drop('Price', axis=1)  # Features
y = data['Price']  # Target variable

# --- One-hot encode categorical features ---
# Create a OneHotEncoder object
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # sparse=False for compatibility with LinearRegression

# Fit the encoder to the categorical columns and transform them
categorical_features = ['Location', 'Garage']  # List your categorical columns here
encoded_features = encoder.fit_transform(X[categorical_features])

# Create a DataFrame with the encoded features
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_features))

# Drop the original categorical columns and concatenate the encoded features
X = X.drop(categorical_features, axis=1)
X = pd.concat([X, encoded_df], axis=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Predict house prices
predicted_prices = model.predict(X_test)

# Calculate RMSE (Root Mean Squared Error) to evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, predicted_prices))
print("ðŸŽ“ Root Mean Squared Error (RMSE):", round(rmse, 2))

import gradio as gr
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# Load dataset (replace 'house_data.csv' with your actual dataset)
# Assuming 'house_price_forecasting_dataset.csv' is your actual dataset file
data = pd.read_csv('house_price_forecasting_dataset.csv')

# Assume the dataset has columns like: 'SquareFootage', 'NumRooms', 'Location', 'YearBuilt', 'Price'
# 'Price' is the target variable we want to predict.

# Replace 'SquareFootage', 'NumRooms', 'Location', 'YearBuilt' with actual columns
# from your 'house_price_forecasting_dataset.csv'
X = data[['Size_sqft', 'Bedrooms', 'Location', 'YearBuilt']]
y = data['Price']

# Handle categorical variables like 'Location' using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the model (Random Forest Regressor in this case)
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Define a scaler for scaling input features before prediction
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Function to predict house prices using user inputs
def predict_price(Size_sqft, Bedrooms, Location, YearBuilt): # Updated parameters
    # Create the input data for prediction
    input_data = {
        'Size_sqft': Size_sqft, # Updated key
        'Bedrooms': Bedrooms, # Updated key
        'Location': Location,
        'YearBuilt': YearBuilt
    }

    # Convert to DataFrame and apply the same transformations as on the training data
    input_df = pd.DataFrame([input_data])
    input_df_encoded = pd.get_dummies(input_df, drop_first=True)
    input_df_encoded = input_df_encoded.reindex(columns=X.columns, fill_value=0)  # Ensure same columns

    # Scale the input data
    scaled_input = scaler.transform(input_df_encoded)

    # Predict using the trained model
    prediction = model.predict(scaled_input)

    return round(prediction[0], 2)

# Create Gradio interface
# Update input components to match features in 'house_price_forecasting_dataset.csv'
# and their respective types (e.g., Dropdown for Location)
iface = gr.Interface(
    fn=predict_price,
    inputs=[
        gr.Number(label="Size (sqft)"),
        gr.Number(label="Number of Bedrooms"),
        gr.Dropdown(choices=data['Location'].unique().tolist(), label="Location"),
        gr.Number(label="Year Built")
    ],
    outputs=gr.Number(label="Predicted House Price (in USD)"),
    live=True
)

iface.launch
